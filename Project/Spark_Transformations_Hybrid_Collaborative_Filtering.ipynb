{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Collaborative Filtering (CF)\n",
        "In this notebook we implement the **hybrid Collabrative Filtering** algorithm to build the recommendation system using PySpark. Our method is composed of the following steps:<br>\n",
        "\n",
        " - **User-based Collabrative Filtering:** Recommending items to the user of intereset based on the preferences of similar users.\n",
        "\n",
        "- **Item-based Collabrative Filtering:** Recommending items to the user of intereset based on the items that similar users have interacted with.\n",
        "\n",
        "- **Merging:** We get the K most similar Item-based and K most similar User-based reccomendations and merge them using the following equation to get the final topK.\n",
        "            - (alpha * item-based) + ( (1−alpha) * user-based)\n",
        "      alpha is a paramater we control for weighting the contribution of each Collabrative Filtering method.\n"
      ],
      "metadata": {
        "id": "AhHuweTmPfj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from itertools import combinations\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import trim, col\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Method to Initialize SparkSession\n",
        "def spark_session_create():\n",
        "    spark_sess = (SparkSession.builder.appName(\"Hybrid_Collaborative_Filtering\")\n",
        "    # number partitions to use when transforming data\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
        "    # Adaptive Query Execution\n",
        "    .config(\"spark.sql.adaptive.enabled\",   \"true\")\n",
        "      # merge small partitions into larger ones when runtime\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "    )\n",
        "    return spark_sess.getOrCreate()"
      ],
      "metadata": {
        "id": "2KZPFNCIyJzK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading and Cleaning\n",
        "- We will use the books data as a lookup to get titles from ISBN.\n",
        "-  We will use ratings data to build the Collabrative Filtering methods."
      ],
      "metadata": {
        "id": "xr4zFDKrUOl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# method to read and clean books and retrn df with ISBN and Title.\n",
        "def read_books(spark, path):\n",
        "\n",
        "    book_temp = (spark.read.option(\"header\", True).option(\"sep\", \";\").csv(path)\n",
        "             # cleaninig\n",
        "             .select(\n",
        "                 trim(col(\"ISBN\")).alias(\"ISBN\"),\n",
        "                 trim(col(\"Title\")).alias(\"Title\")\n",
        "             )\n",
        "             .dropna(subset=[\"ISBN\",\"Title\"])\n",
        "             .dropDuplicates([\"ISBN\"] )\n",
        "    )\n",
        "\n",
        "    return book_temp\n",
        "\n",
        "# read and clean ratings, return df with user, item, rating.\n",
        "def read_ratings(spark, path):\n",
        "\n",
        "    ratings_temp =  (\n",
        "        spark.read.option(\"header\", True).option(\"sep\", \";\").csv(path)\n",
        "        # cleaninig\n",
        "             .select(\n",
        "                 trim(col(\"User-ID\")).alias(\"user\"),\n",
        "                 trim(col(\"ISBN\")).alias(\"item\"),\n",
        "                 col(\"Rating\").cast(\"double\").alias(\"rating\")\n",
        "             )\n",
        "        .filter(col(\"rating\").between(1.0, 10.0))\n",
        "        .dropna()\n",
        "        .dropDuplicates([\"user\",\"item\"] )\n",
        "    )\n",
        "    return ratings_temp"
      ],
      "metadata": {
        "id": "91G21DwLySPB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Centering Ratings by mean\n",
        "We will use the resultings dataframe in calculating the user–user and item–item similarity steps of the pipeline."
      ],
      "metadata": {
        "id": "GkqrTTzoWzIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculte the mean centered ratings retuns centered_df and centered_rdd.\n",
        "def ratings_center_by_mean(ratings_df):\n",
        "\n",
        "    mean_user_rating = ratings_df.groupBy(\"user\").agg(F.avg(\"rating\").alias(\"RatingMean\"))\n",
        "\n",
        "    ratings_center_df = ( ratings_df.join(mean_user_rating, on=\"user\")\n",
        "        # subtract each rating by the mean\n",
        "        .withColumn(\"rating_centered\", col(\"rating\") - col(\"RatingMean\"))\n",
        "        .select(\"user\",\"item\",\"rating\",\"rating_centered\")\n",
        "    )\n",
        "    ratings_center_rdd = ratings_center_df.rdd.map(lambda r: (r[\"user\"], r[\"item\"], r[\"rating_centered\"]))\n",
        "\n",
        "    return ratings_center_df, ratings_center_rdd"
      ],
      "metadata": {
        "id": "pICfP5ayycg0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing\n",
        "\n",
        "To take into account users who rated many books bigger raw dot product with a user who rated smaller number of books we normalize data."
      ],
      "metadata": {
        "id": "H5OE7rwlYfKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the the users normalized and the items normalized\n",
        "def calculate_normalization(ratings_center_rdd):\n",
        "\n",
        "    Users_Normalized = (ratings_center_rdd.map(lambda x: (x[0], x[2]**2))\n",
        "          # sum\n",
        "          .reduceByKey(lambda a, b: a + b)\n",
        "          # squareroot\n",
        "          .mapValues(math.sqrt)\n",
        "          .collectAsMap()\n",
        "    )\n",
        "\n",
        "    Items_Normalized = (ratings_center_rdd.map(lambda x: (x[1], x[2]**2))\n",
        "          .reduceByKey(lambda a, b: a + b)\n",
        "          .mapValues(math.sqrt)\n",
        "          .collectAsMap()\n",
        "    )\n",
        "    return Users_Normalized, Items_Normalized"
      ],
      "metadata": {
        "id": "vZhE5TLvyUYx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine Similarity User-User\n",
        "We only want the users pairs who rated at least one item that the target user ratd."
      ],
      "metadata": {
        "id": "7O_rI_sQbcti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# method to calculate top similar users to  targt  user with cosine similarity\n",
        "# the output is dictionary neighbor_user, similarity.\n",
        "def compute_user_neighbors(ratings_center_rdd, target_user, target_user_items, user_normali, K = 50):\n",
        "\n",
        "    # target user items Ratings\n",
        "    ratings_target = ratings_center_rdd.filter(lambda x: x[1] in target_user_items.value)\n",
        "    ratings_by_item = ratings_target.map(lambda x: (x[1], (x[0], x[2]))).groupByKey()\n",
        "\n",
        "    # the dot prodct parwise\n",
        "    dot_product_user = ratings_by_item.flatMap(lambda kv: [((u_1, u_2), rc_1 * rc_2)\n",
        "                    for (u_1, rc_1),(u_2, rc_2) in combinations(kv[1], 2)]\n",
        "    )\n",
        "\n",
        "    dot_product_user_sum = dot_product_user.reduceByKey(lambda k, kk: k + kk)\n",
        "    dot_product_user_target_user_filter = dot_product_user_sum.filter(lambda kv: target_user in kv[0])\n",
        "\n",
        "    # this is a helper function to use it down in the map\n",
        "    def extracttion(users_pair, dot_pro):\n",
        "        u1,u2 = users_pair\n",
        "        if u1 == target_user:\n",
        "            other_user = u2\n",
        "        else:\n",
        "            other_user = u1\n",
        "        norm_prod = user_normali.value[u1] * user_normali.value[u2] + 1e-8\n",
        "        return other_user, dot_pro / norm_prod\n",
        "\n",
        "    similarities_users = dot_product_user_target_user_filter.map(lambda ab: extracttion(ab[0], ab[1]))\n",
        "    topk = similarities_users.takeOrdered(K, key=lambda x: -x[1])\n",
        "\n",
        "    return dict(topk)"
      ],
      "metadata": {
        "id": "qvvWdjrByh7R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Item-User"
      ],
      "metadata": {
        "id": "2sTPmm70yoDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcu. top items that are similar for each of target user items\n",
        "# output dictionary of item_i1: all other items and sim\n",
        "def calculate_user_items_similarity(ratings_center_rdd, target_items, bc_item_norm, K = 50):\n",
        "\n",
        "    # Ratings for targets items\n",
        "    ratings_target = ratings_center_rdd.filter(lambda a: a[1] in target_items.value)\n",
        "    rating_by_user   = ratings_center_rdd.map(lambda x: (x[0], (x[1], x[2]))).groupByKey()\n",
        "    rating_target_by_user = ratings_target.map(lambda x: (x[0], (x[1], x[2])))\n",
        "    merged = rating_target_by_user.join(rating_by_user)\n",
        "\n",
        "    item_dot_produ = merged.flatMap(\n",
        "        lambda  ab : [((i_1, i_2), rc_1 * rc_2)\n",
        "                    for (i_1, rc_1) in [ab[1][0]]\n",
        "                    for (i_2, rc_2) in ab[1][1]\n",
        "                    if i_2 != i_1 and i_2 not in target_items.value]\n",
        "    )\n",
        "\n",
        "    item_dot_prod_sum = item_dot_produ.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "    item_similaty = item_dot_prod_sum.map(lambda ab: (\n",
        "            ab[0][0],\n",
        "            (ab[0][1], ab[1] / (bc_item_norm.value[ab[0][0]] * bc_item_norm.value[ab[0][1]] + 1e-8))\n",
        "        )\n",
        "    )\n",
        "\n",
        "    similarity_per_item1 = (item_similaty.groupByKey()\n",
        "          .mapValues(lambda iters: sorted(iters, key=lambda x: -x[1])[:K])\n",
        "          .collectAsMap()\n",
        "    )\n",
        "    return similarity_per_item1\n"
      ],
      "metadata": {
        "id": "dlFcifiyymR7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target User Items"
      ],
      "metadata": {
        "id": "GYhiMhzSUxdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get set items that are rated by the target user\n",
        "def get_target_rated_items(rating_center_df, target_user):\n",
        "    tg_items = (rating_center_df\n",
        "          # filter by target user\n",
        "          .filter(col(\"user\") == target_user)\n",
        "          # select items column\n",
        "          .select(\"item\").distinct().rdd.map(lambda r: r[\"item\"]).collect()\n",
        "    )\n",
        "    return set(tg_items)"
      ],
      "metadata": {
        "id": "kxPhOkmIywmg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merged Recommendation"
      ],
      "metadata": {
        "id": "tgBYoP7xU4PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# method to return top-N recommendations hybrid for a  target user.\n",
        "def hybrid_recommendation_system(sc, ratings_df, book_isbn_to_book_title,\n",
        "                      user_neighbors_set, list_neighbors_items,\n",
        "                      target_user, alpha = 0.6, topN = 10):\n",
        "\n",
        "\n",
        "    # turn the df into RDD of user, item, rating\n",
        "    ratings_rdd = ratings_df.rdd.map(lambda row: (row[\"user\"], row[\"item\"], row[\"rating\"]))\n",
        "\n",
        "    # filter the ratings rdd to have only ratings made by the target user\n",
        "    ratings_rdd_target_user = ratings_rdd.filter(lambda row: row[0] == target_user)\n",
        "\n",
        "    # call the item-based collabrative filtering method to get recommedations\n",
        "    item_based_recommendations = (ratings_rdd_target_user\n",
        "           # for each item user rated row[2] look for neighbors make tuple\n",
        "          .flatMap(lambda row: [(item_2, (row[2] * sim, abs(sim)))\n",
        "                              for (item_2, sim) in list_neighbors_items.get(row[1], [])])\n",
        "          # sum contributions weight\n",
        "          .reduceByKey(lambda k1, k2: (k1[0] + k2[0], k1[1] + k2[1]))\n",
        "          # normalize\n",
        "          .mapValues(lambda value: value[0] / (value[1] + 1e-8))\n",
        "          # get dict\n",
        "          .collectAsMap()\n",
        "    )\n",
        "\n",
        "    # call the user-based collabrative filtering method to get recommedations\n",
        "    user_based_recommendations = (ratings_rdd\n",
        "           # kep only ratings by users in the user user neighbors set\n",
        "          .filter(lambda neig: neig[0] in user_neighbors_set)\n",
        "          # for each rating by neighbr, calculate sim\n",
        "          .map(lambda row: (row[1], (row[2] * user_neighbors_set[row[0]], abs(user_neighbors_set[row[0]]))))\n",
        "          # normalization\n",
        "          .reduceByKey(lambda k1, k2: (k1[0] + k2[0], k1[1] + k2[1]))\n",
        "          .mapValues(lambda value: value[0]/(value[1] + 1e-8))\n",
        "          # dict\n",
        "          .collectAsMap()\n",
        "    )\n",
        "\n",
        "    # gather all items\n",
        "    item_recommendations = set(item_based_recommendations) | set(user_based_recommendations)\n",
        "\n",
        "    # calculate hyprid: (alpha * item_based) + (1-alpha * user_based)\n",
        "    hybrids_recommendations = [(recommended_item,\n",
        "        alpha * item_based_recommendations.get(recommended_item, 0.0) + (1 - alpha) * user_based_recommendations.get(recommended_item, 0.0))\n",
        "        for recommended_item in item_recommendations\n",
        "    ]\n",
        "\n",
        "    # sort to get top\n",
        "    top_recommendations = sorted(hybrids_recommendations, key=lambda x: -x[1])[:topN]\n",
        "\n",
        "    # top recommendations has the book isbn and score we want the title also so we look it up in\n",
        "    top_recommendations_with_title = [(book_isbn, book_isbn_to_book_title.get(book_isbn, \"Unknown Title\"), sim_score) for book_isbn, sim_score in top_recommendations]\n",
        "    return top_recommendations_with_title\n"
      ],
      "metadata": {
        "id": "fPd8mD-cy0XJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Code"
      ],
      "metadata": {
        "id": "8xCCAgAkU-ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to gather all the steps above and make recommendation given a user\n",
        "def run_code(target_user,k):\n",
        "    # Initialize spark\n",
        "    spark_sess = spark_session_create()\n",
        "    sc = spark_sess.sparkContext\n",
        "\n",
        "    # Load the data using functions defined\n",
        "    df_books   = read_books(spark_sess, \"Books.csv\")\n",
        "    dataf_ratings = read_ratings(spark_sess, \"Ratings.csv\")\n",
        "\n",
        "    # call the method to make raring centered by mean\n",
        "    rating_centered_df, rating_centered_rdd = ratings_center_by_mean(dataf_ratings)\n",
        "\n",
        "    # get item rated by target user to use them\n",
        "    target_user_rated_items = get_target_rated_items(rating_centered_df, target_user)\n",
        "    target_user_rated_items_brodc = sc.broadcast(target_user_rated_items)\n",
        "\n",
        "    # calculate normalization\n",
        "    map_normalized_users, map_normalized_items = calculate_normalization(rating_centered_rdd)\n",
        "    map_normalized_users_brodc = sc.broadcast(map_normalized_users)\n",
        "    map_normalized_items_brodc = sc.broadcast(map_normalized_items)\n",
        "\n",
        "    # calculate target user similar items\n",
        "    list_user_neighbors = compute_user_neighbors(rating_centered_rdd, target_user, target_user_rated_items_brodc, map_normalized_users_brodc)\n",
        "    list_item_neighbors = calculate_user_items_similarity(rating_centered_rdd, target_user_rated_items_brodc, map_normalized_items_brodc)\n",
        "\n",
        "    # will use in recommendation method to get book title from isbn\n",
        "    book_isbn_to_book_title = dict(df_books.rdd.map(lambda row: (row[\"ISBN\"], row[\"Title\"])) .collect())\n",
        "\n",
        "    # get recomendations\n",
        "    alpha = 0.5\n",
        "    recommendations = hybrid_recommendation_system(sc, dataf_ratings, book_isbn_to_book_title,list_user_neighbors,\n",
        "                         list_item_neighbors,\n",
        "                              target_user,\n",
        "                              alpha,\n",
        "                              k)\n",
        "\n",
        "    print(f\"Top {k} recommendations for user with ID: {target_user}:\")\n",
        "    print(f\"Book ISBN    |    Book Title    |     Score\")\n",
        "    for book_isbn, book_title, sim_score in recommendations:\n",
        "        print(f\"{book_isbn}   |   {book_title}   |   ({sim_score:.4f})\")\n",
        "\n",
        "    # Clean\n",
        "    rating_centered_df.unpersist()\n",
        "    spark_sess.stop()"
      ],
      "metadata": {
        "id": "9fwUwbW9tZS_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "run_code(target_user=\"11676\",k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcqQ80RvuouX",
        "outputId": "2f97579c-f636-46b5-e4eb-c23fbddb21b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 recommendations for user with ID: 11676:\n",
            "Book ISBN    |    Book Title    |     Score\n",
            "04514085856   |   Unknown Title   |   (10.0000)\n",
            "0373029772   |   Ransomed Heart (Harlequin Romance, No 2977)   |   (9.5000)\n",
            "0451158636   |   Devil's Daughter   |   (9.0000)\n",
            "0374302995   |   And If the Moon Could Talk   |   (9.0000)\n",
            "0399134204   |   Joy Luck Club   |   (8.8606)\n",
            "0553289594   |   Among the Shadows: Tales from the Darker Side   |   (8.7561)\n",
            "0679824243   |   Mummies in the Morning (Magic Tree House, Book 3)   |   (8.7380)\n",
            "0060150890   |   Children's Hospital   |   (8.5000)\n",
            "0060932139   |   The Unbearable Lightness of Being : A Novel (Perennial Classics)   |   (8.4201)\n",
            "0671553011   |   HARVEST   |   (8.3635)\n",
            "CPU times: user 2.07 s, sys: 312 ms, total: 2.38 s\n",
            "Wall time: 3min 28s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "run_code(target_user=\"35859\",k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ufVJA6OaIGx",
        "outputId": "31b415cc-610a-4b31-9c32-1cc42d11db78"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 recommendations for user with ID: 35859:\n",
            "Book ISBN    |    Book Title    |     Score\n",
            "0345413903   |   The Murder Book   |   (10.0000)\n",
            "0886774004   |   Arrow's Fall (The Heralds of Valdemar, Book 3)   |   (9.5000)\n",
            "0345338588   |   On a Pale Horse (Incarnations of Immortality, Bk. 1)   |   (9.5000)\n",
            "0671042858   |   The Girl Who Loved Tom Gordon   |   (9.5000)\n",
            "0385502532   |   Drowning Ruth   |   (9.5000)\n",
            "0670835382   |   Four Past Midnight   |   (9.0000)\n",
            "0446602612   |   The Poet   |   (9.0000)\n",
            "0425105334   |   The Talisman   |   (9.0000)\n",
            "0399142851   |   Unnatural Exposure   |   (9.0000)\n",
            "088070683X   |   Love For A Lifetime (mini) : Building A Marriage That Will Go The Distance (A Daily Reminder)   |   (9.0000)\n",
            "CPU times: user 1.14 s, sys: 137 ms, total: 1.28 s\n",
            "Wall time: 1min 41s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "run_code(target_user=\"269566\",k=10)"
      ],
      "metadata": {
        "id": "yAGL92FXaZDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f87d538-5b5e-4a9a-92dd-9b5eaff6c7be"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 recommendations for user with ID: 269566:\n",
            "Book ISBN    |    Book Title    |     Score\n",
            "0486282724   |   Much Ado About Nothing (Dover Thrift Editions)   |   (10.0000)\n",
            "1566192951   |   The Adventures of Huckleberry Finn   |   (9.5000)\n",
            "0441535771   |   Mistress Masham's Repose   |   (9.0000)\n",
            "0812566351   |   Prince of Whales   |   (9.0000)\n",
            "0553265571   |   The Grey Horse   |   (9.0000)\n",
            "0451521358   |   Macbeth   |   (9.0000)\n",
            "0590742590   |   The Second Bend in the River (Point)   |   (9.0000)\n",
            "0448102269   |   The City Mouse and the Country Mouse (Pudgy Pals Series)   |   (9.0000)\n",
            "0563206659   |   Complete Yes Minister   |   (9.0000)\n",
            "031298250X   |   America : A Jake Grafton Novel (A Jake Grafton Novel)   |   (9.0000)\n",
            "CPU times: user 991 ms, sys: 118 ms, total: 1.11 s\n",
            "Wall time: 1min 19s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "run_code(target_user=\"52584\",k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4ft-JvXT7rL",
        "outputId": "f4e2a8aa-d604-4f55-e8e3-26f9b4d146e0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 recommendations for user with ID: 52584:\n",
            "Book ISBN    |    Book Title    |     Score\n",
            "0425176487   |   The Pull of the Moon   |   (10.0000)\n",
            "193156146X   |   The Time Traveler's Wife   |   (9.5000)\n",
            "0440215625   |   Dragonfly in Amber   |   (9.5000)\n",
            "0399147799   |   Death in Paradise (Jesse Stone Novels (Hardcover))   |   (9.0000)\n",
            "0002253941   |   Unknown Title   |   (9.0000)\n",
            "0345337662   |   Interview with the Vampire   |   (9.0000)\n",
            "0345342968   |   Fahrenheit 451   |   (8.5000)\n",
            "0684814994   |   Christmas Box (Christmas Box Trilogy)   |   (8.5000)\n",
            "0345339681   |   The Hobbit : The Enchanting Prelude to The Lord of the Rings   |   (8.5000)\n",
            "0894805770   |   What to Expect the First Year   |   (8.5000)\n",
            "CPU times: user 1.03 s, sys: 185 ms, total: 1.21 s\n",
            "Wall time: 1min 24s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lOQatBSsUB8g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}